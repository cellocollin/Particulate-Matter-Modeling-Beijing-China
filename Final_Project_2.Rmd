---
title: "Final Project: Regression From The Mean"
author: "Zhengtao Xu, Jennings Cheng and Collin Carmichael"
date: "8/6/2021"

output:
  pdf_document: 
    latex_engine: xelatex
    keep_tex: yes
  html_document: default
---


**Section 1: Introduction**

**Section 2: Exploratory Data Analysis**


```{r include=FALSE}
library(lars)
library(sm)
library(leaps)
library(stats)
library(pls)
library("tidyverse")
library("lubridate")
library("ggplot2")
library(ggcorrplot)
library(corrplot)
library(dplyr)
library(ggpubr)
library(plyr)
library(faraway)
library(lmtest)
library(nlme)
library(MASS)
library(Metrics)
```


```{r include=FALSE}
#Read and check data
Pollution<-read.csv("Final Project Data.csv",header=TRUE)
Pollution
head(Pollution)
names(Pollution)
summary(Pollution)
# Change TEMP and PRES as numeric
Pollution$TEMP<-as.numeric(Pollution$TEMP)
Pollution$PRES<-as.numeric(Pollution$PRES)
summary(Pollution)
#Create date-time column for hourly data
Pollution$Date_H=with(Pollution, ymd_h(paste(Year, Month, Day, Hour, sep= ' ')))
Pollution
#Create date column for daily data
Pollution$Date_D=with(Pollution, ymd(paste(Year, Month, Day)))
Pollution[1:5,]
#Hourly Plots
Pollution %>% 
  ggplot(aes(Date_H,PM2.5))+ geom_line()
# Build Daily data
Pollution
Pollution_Daily<-Pollution %>%
  group_by(Date_D) %>%
  summarise(
  Avg_Day_PM2.5=mean(PM2.5,na.rm=TRUE),
  Avg_Day_DEWP=mean(DEWP,na.rm=TRUE),
  Avg_Day_TEMP=mean(TEMP,na.rm=TRUE),
  Avg_Day_PRES=mean(PRES,na.rm=TRUE),
  Max_Day_LWS=max(LWS,na.rm=TRUE),
  Max_Day_LS=max(LS,na.rm=TRUE),
  Max_Day_LR=max(LR,na.rm=TRUE),
  Max_Day_CBWD=nth(CBWD,which.max(LWS)))
Pollution_Daily
# Convert Daily table to a data frame
Pollution_Daily=as.data.frame(Pollution_Daily)
# Check data structure and data summary
str(Pollution_Daily)
summary(Pollution_Daily)
# Daily Plots


#Use functions year, month and day to create Year, Month and Day Columns
#Pollution_Daily$Year=year(Pollution_Daily$Date_D)
#Pollution_Daily$Month=month(Pollution_Daily$Date_D)
#Pollution_Daily$Day=day(Pollution_Daily$Date_D)
#head(Pollution_Daily)
#Fix NaN values in PM2.5 and set to NAs
#Pollution_Daily$Avg_Day_PM2.5[is.nan(Pollution_Daily$Avg_Day_PM2.5)]<-NA
#Pollution_Daily
#Train test


Pollution_Daily<-read.table('Pollution_Daily.csv',header=TRUE)
Pollution_Daily <- as.data.frame(Pollution_Daily)
Pollution_Daily$Year=year(Pollution_Daily$Date_D)
Pollution_Daily$Month=month(Pollution_Daily$Date_D)
Pollution_Daily$Day=day(Pollution_Daily$Date_D)
Pollution_Daily$Avg_Day_PM2.5[is.nan(Pollution_Daily$Avg_Day_PM2.5)]<-NA
Pollution_Daily
Pollution_Daily.test=Pollution_Daily[seq(1,nrow(Pollution_Daily),5), ]
Pollution_Daily.train = Pollution_Daily[-seq(1, nrow(Pollution_Daily) , 5 ) , ]

```
```{r echo=FALSE}
Pollution_Daily_nona<-na.omit(Pollution_Daily)
  #Pollution_Daily_nona
keeps<-c("Avg_Day_PM2.5",'Avg_Day_DEWP','Avg_Day_TEMP','Avg_Day_PRES','Max_Day_LWS' , 'Avg_Day_PRES' 
         , 'Max_Day_LWS' , 'Max_Day_LS' , 'Max_Day_LR')
num_cols <- unlist(lapply(Pollution_Daily_nona, is.numeric))
nump=Pollution_Daily_nona [ , num_cols ]

ggcorrplot(cor(nump))


```
Looking at a correlation heatmap we see aside from dewpoint precipitation and temperature there are no significant correlation in other variable

```{r echo=FALSE}
#Summary stat
#summary(Pollution_Daily)
#9 graph that show distribution of number column
par(mfrow=c(3,3))
hist(Pollution_Daily$Avg_Day_PM2.5)

hist(Pollution_Daily$Avg_Day_DEWP)
hist(Pollution_Daily$Avg_Day_TEMP)
hist(Pollution_Daily$Avg_Day_PRES)
hist(Pollution_Daily$Max_Day_LWS)
hist(Pollution_Daily$Max_Day_LS)
hist(Pollution_Daily$Max_Day_LR)
barplot(prop.table(table(Pollution_Daily$Max_Day_CBWD)))
qqnorm(Pollution_Daily$Avg_Day_PRES)
qqline(Pollution_Daily$Avg_Day_PRES)
```
We can see that PM2.5 has an inverse distribution with values skewed towards low PM2.5 but many high values that go beyond the median

Temp and Dew point are highly related and so may not need to be included in the same model

Precipitation looks like a bell curve but not normal as seen in the plot in 3,3

Wind speed is similar but not identical to PM2.5 in that it has an inverse distribution so it may be highly important in prediction

There is almost always not any snow in Beijing and the max is 27 for a day

Rain is also infrequent but there are times when there is a lot of rain so we may assume that rain and snow are not the same

The most common direction is SE and NW while sometimes NE or CV

Now that we know the distributions we can take a look at some pm2.5 vs predictor


The pm and time is not a clear relationship and there are many seasonal spikes - One thing is that since there are random fluctations in our final model we will not use Date since in a regression model the advance of one day would theoretically advance pm2.5 which is not the relationship we see - and also any hypothesize increase in pm2.5 due to global warming trends in the date would be explained by the year variable and of course the other predictors so we decide not to use the column for our analysis. In the same way we would make month and day factors due to the same concern which is an increase in either of these do not necessarily mean an increase in particulate.

We can also extract that dew, temp, pres are very clearly related from the following which corroborates what was shown in heatmap.

```{r echo=FALSE   , out.height="90%"}
par(mfrow=c(2,2))
plot(1:1826,Pollution_Daily$Avg_Day_TEMP,type="l",xlab="day",ylab='temp')
plot(1:1826,Pollution_Daily$Avg_Day_DEWP,type='l',xlab='day',ylab='dew')
plot(1:1826,Pollution_Daily$Avg_Day_PRES,type='l',xlab='day',ylab='pres')
plot(1:1826,Pollution_Daily$Avg_Day_PM2.5,type='l',xlab='day')
```


PM2.5 is obviously highly unpredictable from just looking at the time series. We will see if there is a relationship with day or month so that we can see if including these in the linear regression as factor would make sense.

```{r echo=FALSE}
pmday=ggplot(Pollution_Daily,aes(Day,Avg_Day_PM2.5))+
geom_line() 
myvar <- c('Avg_Day_PM2.5','Day','Year','Month')

daypm <- Pollution_Daily[myvar]
pollutionnona <- na.omit(daypm)

day2.5=1:31
year2.5=1:4
month2.5=1:12
for (k in 1:5)
{
  year2.5[k] <- mean(pollutionnona$Avg_Day_PM2.5[pollutionnona$Year == k+2009])
}
for (k in 1:31){
day2.5[k] <- mean(pollutionnona$Avg_Day_PM2.5[pollutionnona$Day == k])}
for (k in 1:12)
{
  month2.5[k] <- mean(pollutionnona$Avg_Day_PM2.5[pollutionnona$Month == k])
}
par(mfrow=c(3,1))
plot(1:12 , month2.5 , type = "h" , xlab="month" , ylab='pm2.5 avg '
         )
plot(2010 : 2014, year2.5, type='h' , xlab='year' , ylab='pm2.5 avg')
plot(1:31 , day2.5, type='h' , xlab='day' , ylab= ' pm2.5 avg')
```
As we see the variation between the month year and day is important - highly significant differences between different days, years, or months so it would make much sense to include these in MLR

**3.1: Fitting model and diagnostic**

Now that we know what our data looks like in accordance with the response we can fit a multiple linear regression with response Avg_Day_PM and predictor every variable besides 'Date_D', and look at the results. We can also see the predicted values of pm2.5 in in red on the testing (every 5th observation in the 5 year long data set) after fitting on the training dataset(which is the complement of test)

```{r , echo = FALSE }
##Fitting Full Model
trainingdate <- Pollution_Daily.train [ , - 1 ]
trainingdate
lm_all=lm(Avg_Day_PM2.5~Avg_Day_DEWP+Avg_Day_TEMP+Avg_Day_PRES+Max_Day_LWS+Max_Day_LS+Max_Day_LR+factor(Max_Day_CBWD)+factor(Year)+factor(Month)+factor(Day),data=trainingdate)
summary(lm_all)[4]
row.names(Pollution_Daily.test) <- NULL
Pollution_Daily.test
testpm=predict(lm_all , Pollution_Daily.test [ , -1 ] )

#visually test
#summary(lm_all)
plot(c(1:366) ,  testpm , type = "l" , lwd= "2" , col = "red" )
lines( c(1:366),Pollution_Daily.test[,2])

summary(lm_all)[9]

```
After fitting a full model with every predictor besides 'Date_D' and Month, Year, and Day all in factor version we see that firstly there are many insignificant predictors unsurpisingly in the factors. As expected we see highly significant predictors in the factor(Day) in the 20s since that is what we saw highly in the mean particulate vs. day visualization. But the majority of days are insignificant so it makes sense to just drop the day column. Every month factor is significant besides the 2nd so this is important to keep. Year is all insignificant unless the year is 2012, which means 2012 is a lower pm anomaly. We could hypothetically exclude every non 2012 level but the simpler way is to not use the predictor.

Before considering dropping variables we need to check diagnostics. So here are some of the important assumptions that we will have to validate, namely constant variance, non correlated and normal residuals.


```{r include=FALSE}
##Functions to make Diagnostics Easier
test_error_assumptions<-function(x){
  b=bptest(x)$p.value
  c=shapiro.test(residuals(x))$p.value
  d=dwtest(x)$p.value
  a=data.frame("Homocedasticity",b>0.05,"Normality",c>0.05,"Uncorrelated Errors",d>0.05)
  return(a)
}

test_leverage<-function(x){
n=nrow(model.matrix(x)); p=ncol(model.matrix(x));
lev=influence(x)$hat
a=lev[lev>2*p/n]
  return(a)
}
ncol(model.matrix(lm_all))
test_outlier<-function(x){
  n=nrow(model.matrix(x)); p=ncol(model.matrix(x));
  jack=rstudent(x)
  b=jack[abs(jack)>abs(qt(.05/(2*n), n-p-1))]
  c=abs(qt(.05/(2*n), n-p-1))
  d=">"
  a=data.frame(b,d,c)
  return(a)
}
test_influence<-function(x){
  cook=cooks.distance(x)
  b=cook[cook>1]
  a=data.frame(b)
  return(a)
}
```
We can also check the visualizations for the regression of normality and heteroscedasticity.


```{r echo = FALSE }
par(mfrow=c(2,2))
plot(lm_all)
```
Taking a glance at our linear model, we see that it does not appear to meet all error assumptions.  Particularly, the Q-Q plot suggests non-normality and the Fitted-Residual plot suggests heteroscedasticity and some degree of non-linearity.  The Residuals-Leverage plot however appears to indicate we don't have highly influential points.


```{r echo=FALSE}


diagnosticdata <- as.data.frame(matrix(ncol=1+2,nrow=1))
colnames(diagnosticdata) <- c('bptest p' , 'shapiro test p' , 'durbin watson p')

diagnosticdata[,1] <- bptest(lm_all)[4]
diagnosticdata[,2]<-shapiro.test(residuals(lm_all))[2]
diagnosticdata[,3]<-dwtest(lm_all)[4]
diagnosticdata
```
As we can tell through testing more formally, none of the error assumptions of homocedasticity, normality, and uncorrelated errors are accurate.

```{r echo=FALSE}
##test_influence(lm_all)
##test_leverage(lm_all)
par(mfrow=c(1,2))
##Plot cooks.distance
cook = cooks.distance(lm_all)
halfnorm(cook, labs=row.names(Pollution_Daily.train), ylab="Cook's distances")
##Plot Leverages
lev=influence(lm_all)$hat
halfnorm(lev, labs=row.names(Pollution_Daily.train), ylab="Leverages")
##Checking for cooks.distance greater than 1
# max(cooks.distance(lm_all_2))
##Check For High Leverage Points
# n=nrow(model.matrix(lm_all_2)); p=ncol(model.matrix(lm_all_2));
# lev[lev>2*p/n]

```
In regards to unusual observations, our maximum cooks distance as seen above indicates we have no datapoints that would be classified as highly influential, since it is much less than one.  In constrast, however, we seem to have an abundance of high-leverage points.  However, it is unclear at first glance how many of these are "good" or "bad" leverage points, although we can assume some points like observations 3 and 1468 with wildly different y-values are likely "bad."



```{r echo=FALSE}
test_outlier(lm_all)
# rstudent(lm_all_2)
```
As we can see above, we also have four outliers in training data.

All things aside, in order to remedy non linearity, heteroscedasticity, non normal residual and autocorrelation we can apply a Boxcox transformation first for non linearity and making normally distributed residuals:



```{r echo=FALSE,include=FALSE}
bc=boxcox(lm_all)
lambda=bc$x[which.max(bc$y)]
print(lambda)
```

This is the maximum likelihood for lambda so that will be the transformation we use

We fit the model with the boxcox transformation which involves applying the transformation responseNew=(responseOld^lambda-1)/lambda


```{r echo=FALSE}

par(mfrow=c(2,2))

lmbox <-lm((Avg_Day_PM2.5^lambda-1)/lambda ~Avg_Day_DEWP+Avg_Day_TEMP+Avg_Day_PRES+Max_Day_LWS+Max_Day_LS+Max_Day_LR+factor(Max_Day_CBWD)+factor(Year)+factor(Month)+factor(Day) , data = trainingdate)

diagnosticdata <- as.data.frame(matrix(ncol=1+1+2,nrow=1))
colnames(diagnosticdata) <- c( 'R-square value' , 'bptest p' , 'shapiro test p' , 'durbin watson p')
diagnosticdata[,1+1] <- bptest(lmbox)[4]
diagnosticdata[,2+1]<-shapiro.test(residuals(lmbox))[2]
diagnosticdata[,3+1]<-dwtest(lmbox)[4]
diagnosticdata[,1]<-summary(lmbox)[9]
diagnosticdata
```

The shapiro wilk shows that we easily have residual normal with p=0.54 >.05 while we see that there is autocorrelation via dwtest and heteroscedasticity which is improve - R squared also went up by a massive ~0.1 which means the variation we explain has increased substantially

So our explanation of variation improved but there is still steep autocorrelation and heteroscedasticity - we could combine Generalised Least Squares with correlated error to our Boxcox transformed data for the first one and try a different model with OLS and the Boxcox transformation and evaluate the performance of both model:

```{r echo=FALSE}
##Fitting BoxCox model
general = gls((Avg_Day_PM2.5^.101-1)/.101~., correlation = corAR1(form=~1:1460), data=na.omit(trainingdate),method="ML")
summary(general)
#cant have na's
nonatest=na.omit(Pollution_Daily.test)
nonatest[,2]
#function to reverse the boxcox transformation
invBoxCox <- function(x, lambda)
    		if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda)

row.names(nonatest)<-NULL
#prediction on testing for gls model and compare to lm_all
nonatest[,2]

testgeneral

testgeneral=predict(general,nonatest)
testpm=predict(lm_all,nonatest)
testbox=predict(lmbox,nonatest)

for (k in 1:355)
{
  testgeneral[k]=invBoxCox(testgeneral[k] , lambda );testbox[k]=invBoxCox(testbox[k],lambda)
}

testgeneral

rmsetable=data.frame(rmse(nonatest[,2],testbox),rmse(nonatest[,2],testgeneral) ,
rmse(nonatest[,2],testpm))
colnames(rmsetable)<-c("transormed only r-mse" ,"GLS Transformed r-mse" , "original model r-mse")
rmsetable
intervals(general)[2]
```

As we can see after applying a Generalised Least Square with correlated error with boxcox transformation we have an rmse of 58, which is higher than the original model with every predictor and higher than the transformed only root mean square error. Also we see that we cannot reject the null hypothesis of residual being correlated since the lower bound of Phi which is the correlation variable does not include 0 which is what we would need to reject the null hypothesis. This suggests that GLS may not be the best model for this data since there is still correlation and the r-mse is higher than OLS

Therefore, it may be a good idea to stick with OLS for the sake of fitting a parametric model in this dataset. We noticed that in the GLS model there were two insignificant predictors which were wind direction as a factor and 

```{r}

```

lm32=lm( Avg_Day_PM2.5~.-(Year+Max_Day_CBWD) , data = trainingdate)
bc=boxcox(lm32)
lambda=bc$x[which.max(bc$y)]
lm33=lm((Avg_Day_PM2.5^lambda-1)/lambda ~.-(Year+Max_Day_CBWD) , data = trainingdate)
dwtest(lm33)
shapiro.test(residuals(lm33))
bptest(lm33)
res <- residuals(general)
res[1:1434]

```

```{r echo=FALSE}
g = gls(Avg_Day_PM2.5~.-Date_D, correlation = corARMA(p=1), data=na.omit(Pollution_Daily.train.new))
#summary(g)
lm33=lm((Avg_Day_PM2.5^lambda-1)/lambda ~.-(Year+Max_Day_CBWD) , data = trainingdate)
lm32=lm( Avg_Day_PM2.5~.-(Year+Max_Day_CBWD) , data = trainingdate)
bc=boxcox(lm32)
Pollution_Daily.test
lambda;lm33=lm((Avg_Day_PM2.5^lambda-1)/lambda ~Max_Day_LR+Max_Day_LS+Max_Day_LWS+Avg_Day_PRES+Avg_Day_TEMP+Avg_Day_DEWP+Month+Day  , data = trainingdate)
dim(trainingdate);dim(trainnooutlier)
dwtest(lm33)
shapiro.test(residuals(lm33))
bptest(lm33)
summary(lm33)
lm_all=lm(Avg_Day_PM2.5~., data=trainingdate)
test_outlier(lm_all)
row.names(trainingdate) <- NULL
#trainnooutlier <- trainingdate[-c(333,886,1181, 1213 ) , ]
lmnooutlier <- lm(Avg_Day_PM2.5~.,data=trainnooutlier)
summary(lmnooutlier)
bptest(lm33)
plot(lm33)
nonatest
Pollution_Daily.test
lm_all
bc=boxcox(lm_all)
lambda=bc$x[which.max(bc$y)]
v <- residuals(newgeneralmodel)
attr(v,"std") <- NULL      # get rid of the additional attribute
v=as.vector(v);car::durbinWatsonTest( v )
options(max.print=100000)
bptest(general)
test_error_assumptions(newgeneralmodel)
test_influence(newgeneralmodel)
test_leverage(newgeneralmodel)
test_outlier(newgeneralmodel)
summary(general)
row.names(nonatest) <- NULL
testmodel <- predict(lm33,nonatest)
gen = gls(Avg_Day_PM2.5~.,data=nonatrain)
summary(gen)
intervals(general)
shapiro.test(residuals(newgeneralmodel));dwtest(lm_all)
```

**Section 4: Conclusions**

Through our analysis we have learned many things, such as:
